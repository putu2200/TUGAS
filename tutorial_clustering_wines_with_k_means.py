{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"**Clustering wines with k-means**\"\nauthor: \"Xavier Vivancos Garc√≠a\"\ndate: '`r Sys.Date()`'\noutput: \n  html_document:\n    number_sections: yes\n    toc: yes\n    theme: cosmo\n    highlight: tango\n---\n\n# **Introduction**\n\nk-means is an unsupervised machine learning algorithm used to find groups of observations (clusters) that share similar characteristics. \nWhat is the meaning of unsupervised learning? It means that the observations given in the data set are unlabeled, there is no outcome to be predicted. \nWe are going to use a [Wine data set](https://archive.ics.uci.edu/ml/datasets/wine) to cluster different types of wines. \nThis data set contains the results of a chemical analysis of wines grown in a specific area of Italy. \n\n# **Loading data** {.tabset .tabset-fade .tabset-pills}\n\nFirst we need to load some libraries and read the data set.\n\n```{r message=FALSE, warning=FALSE}\n# Load libraries\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(knitr)\n\n# Read the stats\nwines <- read.csv(\"../input/Wine.csv\")\n```\n\nWe don't need the `Customer_Segment` column. As we have said before, k-means is an unsupervised machine learning algorithm and works with unlabeled data. \n\n```{r}\n# Remove the Type column\nwines <- wines[, -14]\n```\n\nLet's get an idea of what we're working with.\n\n## First rows \n```{r}\n# First rows \nkable(head(wines))\n```\n\n## Last rows \n```{r}\n# Last rows \nkable(tail(wines))\n```\n\n## Summary \n```{r}\n# Summary\nkable(summary(wines))\n```\n\n## Structure\n```{r}\n# Structure \nstr(wines)\n```\n\n# **Data analysis** \n\nFirst we have to explore and visualize the data.\n\n```{r message=FALSE, fig.align='center'}\n# Histogram for each Attribute\nwines %>%\n  gather(Attributes, value, 1:13) %>%\n  ggplot(aes(x=value, fill=Attributes)) +\n  geom_histogram(colour=\"black\", show.legend=FALSE) +\n  facet_wrap(~Attributes, scales=\"free_x\") +\n  labs(x=\"Values\", y=\"Frequency\",\n       title=\"Wines Attributes - Histograms\") +\n  theme_bw()\n```\n\n```{r message=FALSE, fig.align='center'}\n# Density plot for each Attribute\nwines %>%\n  gather(Attributes, value, 1:13) %>%\n  ggplot(aes(x=value, fill=Attributes)) +\n  geom_density(colour=\"black\", alpha=0.5, show.legend=FALSE) +\n  facet_wrap(~Attributes, scales=\"free_x\") +\n  labs(x=\"Values\", y=\"Density\",\n       title=\"Wines Attributes - Density plots\") +\n  theme_bw()\n```\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Boxplot for each Attribute  \nwines %>%\n  gather(Attributes, values, c(1:4, 6:12)) %>%\n  ggplot(aes(x=reorder(Attributes, values, FUN=median), y=values, fill=Attributes)) +\n  geom_boxplot(show.legend=FALSE) +\n  labs(title=\"Wines Attributes - Boxplots\") +\n  theme_bw() +\n  theme(axis.title.y=element_blank(),\n        axis.title.x=element_blank()) +\n  ylim(0, 35) +\n  coord_flip()\n```\n\nWe haven't included magnesium and proline, since their values are very high and worsen the visualization.\n\nWhat is the relationship between the different attributes? We can use the `corrplot()` function to create a graphical display of a correlation matrix. \n\n```{r fig.align='center'}\n# Correlation matrix \ncorrplot(cor(wines), type=\"upper\", method=\"ellipse\", tl.cex=0.9)\n```\n\nThere is a strong linear correlation between `Total_Phenols` and `Flavanoids`. We can model the relationship between these two variables by fitting a linear equation.\n\n```{r fig.align='center'}\n# Relationship between Phenols and Flavanoids\nggplot(wines, aes(x=Total_Phenols, y=Flavanoids)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(title=\"Wines Attributes\",\n       subtitle=\"Relationship between Phenols and Flavanoids\") +\n  theme_bw()\n```\n\nNow that we have done a exploratory data analysis, we can prepare the data in order to execute the k-means algorithm. \n\n# **Data preparation** \n\nWe have to normalize the variables to express them in the same range of values. In other words, normalization means adjusting values measured on different scales to a common scale.\n\n```{r fig.align='center'}\n# Normalization\nwinesNorm <- as.data.frame(scale(wines))\n\n# Original data\np1 <- ggplot(wines, aes(x=Alcohol, y=Malic_Acid)) +\n  geom_point() +\n  labs(title=\"Original data\") +\n  theme_bw()\n\n# Normalized data \np2 <- ggplot(winesNorm, aes(x=Alcohol, y=Malic_Acid)) +\n  geom_point() +\n  labs(title=\"Normalized data\") +\n  theme_bw()\n\n# Subplot\ngrid.arrange(p1, p2, ncol=2)\n```\n\nThe points in the normalized data are the same as the original one. The only thing that changes is the scale of the axis.\n\n# **k-means execution** \n\nIn this section we are going to execute the k-means algorithm and analyze the main components that the function returns. \n\n```{r}\n# Execution of k-means with k=2\nset.seed(1234)\nwines_k2 <- kmeans(winesNorm, centers=2)\n```\n\nThe `kmeans()` function returns an object of class \"`kmeans`\" with information about the partition: \n\n* `cluster`. A vector of integers indicating the cluster to which each point is allocated.\n\n* `centers`. A matrix of cluster centers.\n\n* `size`. The number of points in each cluster.\n\n```{r}\n# Cluster to which each point is allocated\nwines_k2$cluster\n\n# Cluster centers\nwines_k2$centers\n\n# Cluster size\nwines_k2$size\n```\n\nAdditionally, the `kmeans()` function returns some ratios that let us know how compact is a cluster and how different are several clusters among themselves. \n\n* `betweenss`. The between-cluster sum of squares. In an optimal segmentation, one expects this ratio to be as higher as possible, since we would like to have heterogeneous clusters.\n\n* `withinss`. Vector of within-cluster sum of squares, one component per cluster. In an optimal segmentation, one expects this ratio to be as lower as possible for each cluster, \n* since we would like to have homogeneity within the clusters.\n\n* `tot.withinss`. Total within-cluster sum of squares. \n\n* `totss`. The total sum of squares.\n\n```{r}\n# Between-cluster sum of squares\nwines_k2$betweenss\n\n# Within-cluster sum of squares\nwines_k2$withinss\n\n# Total within-cluster sum of squares \nwines_k2$tot.withinss\n\n# Total sum of squares\nwines_k2$totss\n```\n\n# **How many clusters?**\n\nTo study graphically which value of `k` gives us the best partition, we can plot `betweenss` and `tot.withinss` vs Choice of `k`. \n\n```{r fig.align='center'}\nbss <- numeric()\nwss <- numeric()\n\n# Run the algorithm for different values of k \nset.seed(1234)\n\nfor(i in 1:10){\n\n  # For each k, calculate betweenss and tot.withinss\n  bss[i] <- kmeans(winesNorm, centers=i)$betweenss\n  wss[i] <- kmeans(winesNorm, centers=i)$tot.withinss\n\n}\n\n# Between-cluster sum of squares vs Choice of k\np3 <- qplot(1:10, bss, geom=c(\"point\", \"line\"), \n            xlab=\"Number of clusters\", ylab=\"Between-cluster sum of squares\") +\n  scale_x_continuous(breaks=seq(0, 10, 1)) +\n  theme_bw()\n\n# Total within-cluster sum of squares vs Choice of k\np4 <- qplot(1:10, wss, geom=c(\"point\", \"line\"),\n            xlab=\"Number of clusters\", ylab=\"Total within-cluster sum of squares\") +\n  scale_x_continuous(breaks=seq(0, 10, 1)) +\n  theme_bw()\n\n# Subplot\ngrid.arrange(p3, p4, ncol=2)\n```\n\nWhich is the optimal value for `k`? One should choose a number of clusters so that adding another cluster doesn't give much better partition of the data. At some point the gain will drop, giving an angle in the graph (elbow criterion). \nThe number of clusters is chosen at this point. In our case, it is clear that 3 is the appropriate value for `k`. \n\n# **Results**\n\n```{r fig.align='center'}\n# Execution of k-means with k=3\nset.seed(1234)\n\nwines_k3 <- kmeans(winesNorm, centers=3)\n\n# Mean values of each cluster\naggregate(wines, by=list(wines_k3$cluster), mean)\n\n# Clustering \nggpairs(cbind(wines, Cluster=as.factor(wines_k3$cluster)),\n        columns=1:6, aes(colour=Cluster, alpha=0.5),\n        lower=list(continuous=\"points\"),\n        upper=list(continuous=\"blank\"),\n        axisLabels=\"none\", switch=\"both\") +\n        theme_bw()\n```\n\n# **Summary** \n\nIn this entry we have learned about the k-means algorithm, including the data normalization before we execute it, the choice of the optimal number of clusters (elbow criterion) and the visualization of the clustering.\n\nIt has been a pleasure to make this post, I have learned a lot! Thank you for reading and if you like it, please upvote it.\n\n# **Citations for used packages**\n\nDua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n\nHadley Wickham (2017). tidyverse: Easily Install and Load the 'Tidyverse'. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\n\nTaiyun Wei and Viliam Simko (2017). R package \"corrplot\": Visualization of a Correlation Matrix (Version 0.84). Available from https://github.com/taiyun/corrplot\n\nBaptiste Auguie (2017). gridExtra: Miscellaneous Functions for \"Grid\" Graphics. R package version 2.3. https://CRAN.R-project.org/package=gridExtra\n\nBarret Schloerke, Jason Crowley, Di Cook, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg and Joseph Larmarange (2017). GGally: Extension to 'ggplot2'. R package version 1.3.2. https://CRAN.R-project.org/package=GGally\n\nYihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.20.\n\nYihui Xie (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC. ISBN 978-1498716963\n\nYihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595\n","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}